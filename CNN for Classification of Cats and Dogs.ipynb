{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks for Classification of Cats and Dogs\n",
    "###### Author: Rantete Tshinanga Keith\n",
    "\n",
    "#### This is a classification problem that employs machine vision, where the CNNs, after some intense training, will be able to classify cats and dogs, given their images. The model learns from scratch, via supervised machine learning.\n",
    "#### About 8000 and 2000 images of cats and dogs will be used in the training phase and testing phase respectively.\n",
    "\n",
    "#### Optimizer: \n",
    "###### Adaptive Moment Estimation (ADAM) so our stochastic gradient descent optimizes to a global minima. Both the weights and the feature detectors are optimized during the backpropagation optimization.\n",
    "#### Loss function:\n",
    "##### Binary CrossEntropy since binary encoding of the target categories will be employed.\n",
    "#### Activation function (convolution):\n",
    "##### Rectified Linear Unit (RELU) to introduce non-linearities to the images.\n",
    "#### Activation function (voting synapse):\n",
    "##### Sigmoid function, optimal for binary encoded categories.\n",
    "#### Stages: Images inputs, convolution, max pooling, convolution, max pooling, flattening, image inputs, fully connected neurons, output (a cat or a dog)\n",
    "#### Convolution: \n",
    "##### Images, feature detectors, feature maps\n",
    "#### Max pooling:\n",
    "##### To add spatial invariance, prevent overfitting, and for dimension reduction of the images.\n",
    "#### Flattening:\n",
    "##### To flatten the tensors into a one-dimensional vector to serve as an input to the Fully Connected Artifical Neural Networks.\n",
    "#### Full connection:\n",
    "##### Full connection of neural layers and neurons respectively.\n",
    "###### Using similar procedure, we can build an AI that can diagnose patients after diagnostic imaging tests such as X-rays, CT scan, MRI, Mammogram, Ultrasound, Fluoroscopy, and PET scans. The AI can improve with time to even make more accurate diagnosis than a human doctor can, thus removing human error in medical diagnosis.\n",
    "\n",
    "###### More applications of the CNNs are in self-driving cars, where the CNNs enable car vision so it can recognize road signs, lanes, other cars and the like, which allows a car to drive itself and may help in reducing accidents caused by human error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import sys\n",
    "from PIL import Image\n",
    "sys.modules['Image'] = Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "## Image Argumentation to avoid overfitting\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,   #applies feature scaling to pixel by dividng each pixel by 255\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        'dataset/training_set',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        'dataset/test_set',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialising the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a second convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid')) \n",
    "# sigmoid for binary classification and softmax for multiple classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the CNN on the Training set and evaluating it on the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 359s 1s/step - loss: 0.2279 - accuracy: 0.9087 - val_loss: 0.5157 - val_accuracy: 0.7960\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 126s 505ms/step - loss: 0.2157 - accuracy: 0.9120 - val_loss: 0.5451 - val_accuracy: 0.7885\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 46s 182ms/step - loss: 0.1994 - accuracy: 0.9204 - val_loss: 0.6090 - val_accuracy: 0.7885\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 47s 186ms/step - loss: 0.1957 - accuracy: 0.9210 - val_loss: 0.6078 - val_accuracy: 0.7895\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 45s 178ms/step - loss: 0.1888 - accuracy: 0.9225 - val_loss: 0.5735 - val_accuracy: 0.7895\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 46s 186ms/step - loss: 0.1883 - accuracy: 0.9245 - val_loss: 0.5913 - val_accuracy: 0.7890\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 44s 175ms/step - loss: 0.1671 - accuracy: 0.9344 - val_loss: 0.6003 - val_accuracy: 0.7880\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 45s 178ms/step - loss: 0.1626 - accuracy: 0.9354 - val_loss: 0.6657 - val_accuracy: 0.7780\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 0.1605 - accuracy: 0.9324 - val_loss: 0.6639 - val_accuracy: 0.7845\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 49s 197ms/step - loss: 0.1467 - accuracy: 0.9395 - val_loss: 0.7348 - val_accuracy: 0.7825\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 49s 194ms/step - loss: 0.1414 - accuracy: 0.9456 - val_loss: 0.7184 - val_accuracy: 0.7905\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 53s 212ms/step - loss: 0.1302 - accuracy: 0.9511 - val_loss: 0.6932 - val_accuracy: 0.7950\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 49s 195ms/step - loss: 0.1340 - accuracy: 0.9484 - val_loss: 0.7560 - val_accuracy: 0.7815\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 50s 199ms/step - loss: 0.1291 - accuracy: 0.9516 - val_loss: 0.6953 - val_accuracy: 0.8020\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 0.1233 - accuracy: 0.9507 - val_loss: 0.7381 - val_accuracy: 0.7935\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 0.1130 - accuracy: 0.9588 - val_loss: 0.7527 - val_accuracy: 0.7920\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 0.1132 - accuracy: 0.9563 - val_loss: 0.7393 - val_accuracy: 0.8070\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 0.1174 - accuracy: 0.9546 - val_loss: 0.8071 - val_accuracy: 0.7855\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 47s 190ms/step - loss: 0.1150 - accuracy: 0.9571 - val_loss: 0.8004 - val_accuracy: 0.7810\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 0.1174 - accuracy: 0.9582 - val_loss: 0.7316 - val_accuracy: 0.7890\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 0.1002 - accuracy: 0.9636 - val_loss: 0.7959 - val_accuracy: 0.7945\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 49s 194ms/step - loss: 0.0934 - accuracy: 0.9668 - val_loss: 0.7661 - val_accuracy: 0.7950\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 0.0938 - accuracy: 0.9663 - val_loss: 0.7641 - val_accuracy: 0.7920\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 47s 190ms/step - loss: 0.0952 - accuracy: 0.9651 - val_loss: 0.8607 - val_accuracy: 0.7820\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 0.0916 - accuracy: 0.9636 - val_loss: 0.8931 - val_accuracy: 0.7880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c02232b610>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x = training_set, validation_data = test_set, epochs = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('dataset/single_prediction/cat_or_dog_3.jfif', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1:\n",
    "  prediction = 'Dog'\n",
    "else:\n",
    "  prediction = 'Cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('dataset/single_prediction/cat_or_dog_4.jfif', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1:\n",
    "  prediction = 'Dog'\n",
    "else:\n",
    "  prediction = 'Cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dog\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
